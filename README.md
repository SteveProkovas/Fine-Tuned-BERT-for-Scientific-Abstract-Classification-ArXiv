# ğŸ“„ Fine-Tuned BERT for Scientific Abstract Classification (ArXiv)

[![license](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](#)
[![python](https://img.shields.io/badge/python-3.8%2B-orange.svg)](#)
[![pytorch](https://img.shields.io/badge/pytorch-%3E%3D1.10-red.svg)](#)

> Fine-tune a Transformer model to classify ArXiv paper abstracts into subject categories (Computer Science, Mathematics, Physics, etc.). This repo is GitHub-ready and includes training, evaluation, a Colab notebook, and a SciBERT variant tuned for scientific text.

---

## ğŸš€ Quick Summary

This repository demonstrates a production-friendly pipeline to fine-tune a Transformer (default: **SciBERT** / `allenai/scibert_scivocab_uncased`) for **multi-class classification** on the ArXiv abstracts dataset. The project focuses on reproducibility, sensible defaults, and easy experimentation.

**Outcomes:**

* Strong baseline: TF-IDF + Logistic Regression
* Transformer baseline: SciBERT / `bert-base-uncased` fine-tuned for classification
* Metrics: Accuracy, Macro-F1, Weighted-F1, Confusion matrix

---

## âœ… Features

* Modular training script with CLI arguments
* Data preprocessing & label mapping utilities
* Model checkpointing, early stopping, and reproducible seeds
* Evaluation script that saves metrics & plots (confusion matrix)
* Optional use of `scibert` for scientific text
* Colab-ready notebook for quick experimentation
* Dockerfile for environment reproducibility
* Continuous Integration test (lightweight smoke test)

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ download_arxiv.py      # (optional) script to download & preprocess
â”‚   â””â”€â”€ processed/             # final TSV/JSONL splits
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data.py                # Dataset class, tokenization, collators
â”‚   â”œâ”€â”€ model.py               # Model wrapper (HF transformers)
â”‚   â”œâ”€â”€ train.py               # Training entrypoint
â”‚   â”œâ”€â”€ evaluate.py            # Evaluation entrypoint
â”‚   â”œâ”€â”€ utils.py               # helpers: metrics, seed, IO
â”‚   â””â”€â”€ predict.py             # Inference script for single abstract
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ colab_experiment.ipynb # runnable Colab notebook
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ entrypoint.sh
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/ci.yml       # lightweight CI tests
â””â”€â”€ saved_models/
    â””â”€â”€ bert-arxiv/            # example checkpoint structure
```

---

## ğŸ”§ Requirements

Prefer a conda environment (example `env.yml` included)

* Python 3.8+
* PyTorch (GPU recommended)
* `transformers` (Hugging Face)
* `datasets` (Hugging Face datasets)
* `scikit-learn`, `numpy`, `pandas`, `tqdm`, `matplotlib`

Example install:

```bash
python -m pip install -r requirements.txt
```

---

## ğŸ“¥ Data (Download & Preprocess)

1. Download the [ArXiv dataset (Kaggle / Cornell)](#). Save as `dataset/raw/arxiv.csv`.
2. Run the preprocessing script to extract `abstract`, `primary_category`, and create splits:

```bash
python dataset/download_arxiv.py --input dataset/raw/arxiv.csv --out_dir dataset/processed --split_ratio 0.8 0.1 0.1
```

`download_arxiv.py` will:

* Normalize categories (e.g., map subcategories to top-level categories)
* Remove extremely short or empty abstracts
* Create `train.jsonl`, `val.jsonl`, `test.jsonl` with fields `{id, title, abstract, label, label_id}`

**Label mapping:** A `label_map.json` is saved with `{label_name: label_id}` to guarantee stable mapping between runs.

---

## ğŸ‹ï¸â€â™€ï¸ Training (example)

Default uses SciBERT. To train with SciBERT (recommended for scientific text):

```bash
python src/train.py \
  --train_file dataset/processed/train.jsonl \
  --val_file dataset/processed/val.jsonl \
  --model_name_or_path allenai/scibert_scivocab_uncased \
  --output_dir saved_models/bert-arxiv \
  --num_labels 10 \
  --max_length 256 \
  --batch_size 16 \
  --epochs 4 \
  --lr 2e-5 \
  --seed 42
```

Key behaviors implemented in the training script:

* Mixed-precision if supported (AMP)
* Gradient accumulation for large effective batch sizes
* Checkpointing on best validation Macro-F1
* Logging to `training.log` and saving a `training_config.json` alongside the model

---

## ğŸ“Š Evaluation

Run evaluation (generates metrics JSON and confusion matrix PNG):

```bash
python src/evaluate.py --model_dir saved_models/bert-arxiv --test_file dataset/processed/test.jsonl --out_dir results/
```

Outputs:

* `results/metrics.json` (accuracy, f1\_macro, f1\_weighted, precision, recall)
* `results/confusion_matrix.png`
* `results/predictions.csv` (id, true\_label, pred\_label, pred\_probs)

---

## ğŸ§¾ Example CLI Options (train.py)

* `--model_name_or_path` (str) â€” HF model name (default `allenai/scibert_scivocab_uncased`)
* `--max_length` (int) â€” tokenizer max length (default 256)
* `--batch_size` (int)
* `--epochs` (int)
* `--lr` (float)
* `--weight_decay` (float)
* `--warmup_ratio` (float)
* `--early_stopping_patience` (int)

---

## â™»ï¸ Reproducibility

* Seed is set across Python, NumPy, and PyTorch
* Environment recorded in `requirements.txt` and `training_config.json`
* Model deterministic flags where possible (note: exact GPU nondeterminism may vary)

---

## ğŸ§© Tips & Ablations

* Try `roberta-base` and `allenai/scibert_scivocab_uncased` and compare Macro-F1
* Increase `max_length` to 512 for longer abstracts (slower/training cost up)
* Use class weighting or focal loss if label distribution is skewed
* Use `stratified` splits by label for stability in small classes

---

## ğŸ§ª CI / Tests

A lightweight test runs a single forward pass on a tiny synthetic batch to validate dependencies and imports. This is used in `.github/workflows/ci.yml`.

---

## ğŸ““ Colab Notebook

`notebooks/colab_experiment.ipynb` includes:

* Dataset preview (first 100 examples)
* One-click model training (small subset for demo)
* Evaluation and visualizations

A downloadable copy of the notebook is included so users can launch on Colab with the `Open in Colab` badge.

---

## ğŸ³ Docker

Dockerfile builds a reproducible image with GPU-capable base (nvidia/cuda) for training. `docker/entrypoint.sh` wraps the training command.

---

## âœï¸ Contribution Guide

* Open an issue for feature requests or bugs
* Fork, create a feature branch, and open a PR with tests/docs
* Follow code style: `black` + `flake8`

---

## ğŸ“š References & Acknowledgments

This project builds on top of Hugging Face Transformers and the ArXiv dataset. SciBERT is recommended for best results on scientific text.

---

## ğŸ§¾ License

Apache 2.0

---
